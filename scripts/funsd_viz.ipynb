{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,'/home/prashant/unilm/layoutlmv3/examples/')\n",
    "from gat_utils import *\n",
    "# from ..examples.gat_utils import *\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datasets import load_metric \n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pickle.load(open('/home/prashant/unilm/layoutlmv3/debug/funsd_train_processed_dataset.pkl','rb'))\n",
    "test_dataset = pickle.load(open('/home/prashant/unilm/layoutlmv3/debug/funsd_test_processed_dataset.pkl','rb'))\n",
    "# test_dataset_pre = pickle.load(open('/data1/prashant/DocIE/debug_v2/test_dataset_pre.pkl','rb'))\n",
    "\n",
    "# id_to_image_test_json_ = json.load(open('/home/prashant/unilm/layoutlmv3/data/id_to_image_test.json'))\n",
    "# id_to_image_test_json = {int(k):v for k,v in id_to_image_test_json_.items()}\n",
    "image_to_id_test_json = json.load(open('/home/prashant/unilm/layoutlmv3/data/image_to_id_test.json'))\n",
    "id_to_image_test_json = {j:i for i, j in image_to_id_test_json.items()}\n",
    "\n",
    "# id_to_image_eval_json_ = json.load(open('/home/prashant/unilm/layoutlmv3/data/id_to_image_eval_funsd.json'))\n",
    "# id_to_image_eval_json = {int(k):v for k,v in id_to_image_eval_json_.items()}\n",
    "# image_to_id_eval_json = {j:i for i, j in id_to_image_eval_json.items()}\n",
    "\n",
    "image_to_id_train_json = json.load(open('/home/prashant/unilm/layoutlmv3/data/image_to_id_train.json'))\n",
    "# image_to_id_train_json = {int(k):v for k,v in id_to_image_train_json_.items()}\n",
    "id_to_image_train_json = {int(j):i for i, j in image_to_id_train_json.items()}\n",
    "\n",
    "few_shot_info = json.loads(open('/home/prashant/unilm/layoutlmv3/data/funsd_few_shot_info.json','r').read())\n",
    "# image_to_id_dict = json.loads(open('/data1/prashant/DocIE/data/image_to_id.json','r').read())\n",
    "\n",
    "# datasets = pickle.load(open('/data1/prashant/DocIE/data/datasets.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_height_train, width_height_test = get_widths_heights(id_to_image_train_json, id_to_image_test_json)\n",
    "width_height_list = [width_height_test[i] for i in test_dataset['overflow_to_sample_mapping']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filewise_metrics_csv(test_predictions, true_labels):\n",
    "    filewise_metrics_csv = pd.DataFrame()\n",
    "    row = defaultdict(float)\n",
    "    tag_spec = ['precision','recall','f1','number']\n",
    "    i = 0\n",
    "    for preds, labels in zip(test_predictions, true_labels):\n",
    "        row['id'] = i\n",
    "        result = metric.compute(predictions=[preds], references=[labels])\n",
    "        for k in ['ANSWER','QUESTION','HEADER']:\n",
    "            for x in tag_spec:\n",
    "                    try:\n",
    "                        row[f\"{k}_{x}\"] = result[k][x]\n",
    "                    except:\n",
    "                        row[f\"{k}_{x}\"] = 0\n",
    "        row['overall_precision'] = result['overall_precision']\n",
    "        row['overall_recall'] = result['overall_precision']\n",
    "        row['overall_f1'] = result['overall_f1']\n",
    "        row['overall_accuracy'] = result['overall_accuracy']\n",
    "        filewise_metrics_csv = filewise_metrics_csv.append(row, ignore_index=True)\n",
    "        i+=1\n",
    "    return filewise_metrics_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.genfromtxt('/home/prashant/unilm/layoutlmv3/data/funsd_true_labels.txt', dtype='str',delimiter=',,,')\n",
    "true_labels_1 = [eval(x) for x in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sz  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/anaconda3/envs/layoutlmv3/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/prashant/anaconda3/envs/layoutlmv3/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/prashant/anaconda3/envs/layoutlmv3/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/prashant/anaconda3/envs/layoutlmv3/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/prashant/anaconda3/envs/layoutlmv3/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/prashant/anaconda3/envs/layoutlmv3/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 30, 14, 31, 31, 35]\n",
      "[37, 33, 17, 24, 24, 28]\n",
      "Sz  2\n",
      "[33, 40, 37, 34, 38, 33]\n",
      "[31, 36, 35, 28, 36, 23]\n",
      "Sz  3\n",
      "[37, 39, 40, 31, 39, 32]\n",
      "[37, 38, 39, 18, 34, 29]\n",
      "Sz  4\n",
      "[38, 37, 38, 26, 41, 26]\n",
      "[36, 34, 33, 19, 40, 20]\n",
      "Sz  5\n",
      "[39, 34, 35, 26, 39, 27]\n",
      "[37, 41, 35, 26, 33, 32]\n",
      "Sz  6\n",
      "[29, 36, 46, 26, 38, 23]\n",
      "[26, 35, 41, 27, 41, 23]\n",
      "Sz  7\n",
      "[32, 37, 34, 41, 33, 30]\n",
      "[31, 43, 33, 38, 34, 28]\n",
      "Sz  8\n",
      "[35, 30, 32, 39, 37, 40]\n",
      "[31, 34, 34, 35, 33, 44]\n",
      "Sz  9\n",
      "[27, 36, 37, 38, 26, 38]\n",
      "[26, 41, 37, 26, 32, 35]\n",
      "Sz  10\n",
      "[27, 37, 29, 35, 40, 31]\n",
      "[29, 37, 35, 36, 42, 30]\n"
     ]
    }
   ],
   "source": [
    "sizes = [1,2,3,4,5,6,7,8,9,10]\n",
    "seeds = [0,1,2,3,4,5]\n",
    "true_labels = np.genfromtxt('/home/prashant/unilm/layoutlmv3/data/funsd_true_labels.txt', dtype='str',delimiter=',,,')\n",
    "true_labels_1 = [eval(x) for x in true_labels]\n",
    "gat_closest_counts, gat_angles_counts = [],[]\n",
    "for sz in sizes:\n",
    "    print(\"Sz \", sz)\n",
    "    closest_counts, angles_counts = [],[]\n",
    "    for sd in seeds:\n",
    "        test_fs_preds = np.genfromtxt(f'/home/prashant/unilm/layoutlmv3/results/test-layoutlmv3-fs-{sz}-{sd}/test_predictions.txt',dtype='str',delimiter=',,,,')\n",
    "        test_gat_closest_preds = np.genfromtxt(f'/home/prashant/unilm/layoutlmv3/results/test-layoutlmv3-gat-closest-{sz}-{sd}-h4-d4/test_predictions.txt',dtype='str',delimiter=',,,,')\n",
    "        test_gat_angles_preds = np.genfromtxt(f'/home/prashant/unilm/layoutlmv3/results/test-layoutlmv3-gat-60-angles-v2-{sz}-{sd}-h4-d4/test_predictions.txt',dtype='str',delimiter=',,,,')\n",
    "        test_fs_preds = [x.split() for x in test_fs_preds]\n",
    "        test_gat_closest_preds = [x.split() for x in test_gat_closest_preds]\n",
    "        test_gat_angles_preds = [x.split() for x in test_gat_angles_preds]\n",
    "        filewise_baseline = get_filewise_metrics_csv(test_fs_preds, true_labels_1)\n",
    "        filewise_gat_closest = get_filewise_metrics_csv(test_gat_closest_preds, true_labels_1)\n",
    "        filewise_gat_angles = get_filewise_metrics_csv(test_gat_angles_preds, true_labels_1)\n",
    "        closest_counts.append(sum(filewise_baseline['overall_f1'] <= filewise_gat_closest['overall_f1']))\n",
    "        angles_counts.append(sum(filewise_baseline['overall_f1'] <= filewise_gat_angles['overall_f1']))\n",
    "    print(closest_counts)\n",
    "    print(angles_counts)\n",
    "    gat_closest_counts.append(np.mean(closest_counts))\n",
    "    gat_angles_counts.append(np.mean(angles_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNSD - closest\n",
      "Sz : 1 31.5 0.6057692307692307\n",
      "Sz : 2 35.833333333333336 0.6891025641025641\n",
      "Sz : 3 36.333333333333336 0.6987179487179488\n",
      "Sz : 4 34.333333333333336 0.6602564102564104\n",
      "Sz : 5 33.333333333333336 0.6410256410256411\n",
      "Sz : 6 33.0 0.6346153846153846\n",
      "Sz : 7 34.5 0.6634615384615384\n",
      "Sz : 8 35.5 0.6826923076923077\n",
      "Sz : 9 33.666666666666664 0.6474358974358974\n",
      "Sz : 10 33.166666666666664 0.6378205128205128\n",
      "AVG:  0.6616809116809116\n",
      "FUNSD - angles\n",
      "Sz : 1 27.166666666666668 0.5224358974358975\n",
      "Sz : 2 31.5 0.6057692307692307\n",
      "Sz : 3 32.5 0.625\n",
      "Sz : 4 30.333333333333332 0.5833333333333333\n",
      "Sz : 5 34.0 0.6538461538461539\n",
      "Sz : 6 32.166666666666664 0.6185897435897435\n",
      "Sz : 7 34.5 0.6634615384615384\n",
      "Sz : 8 35.166666666666664 0.6762820512820512\n",
      "Sz : 9 32.833333333333336 0.6314102564102565\n",
      "Sz : 10 34.833333333333336 0.6698717948717949\n",
      "AVG:  0.6363960113960113\n"
     ]
    }
   ],
   "source": [
    "print(\"FUNSD - closest\")\n",
    "avgs = []\n",
    "for i, avg in enumerate(gat_closest_counts):\n",
    "    print(f'Sz : {i+1} {avg} {avg/52}')\n",
    "    avgs.append(avg/52)\n",
    "print(\"AVG: \",np.mean(avgs[1:]))\n",
    "\n",
    "avgs = []\n",
    "print(\"FUNSD - angles\")\n",
    "for i, avg in enumerate(gat_angles_counts):\n",
    "    print(f'Sz : {i+1} {avg} {avg/52}')\n",
    "    avgs.append(avg/52)\n",
    "print(\"AVG: \",np.mean(avgs[1:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutlmv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "644fde070b247895b3519e17c816c014a26f42734ecc5c6e8372bedda82249ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
